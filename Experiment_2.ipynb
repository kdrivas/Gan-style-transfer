{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import sys\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from model import generator\n",
    "from model import discriminator_cnn\n",
    "import helpers\n",
    "from data_iter import DisDataIter\n",
    "\n",
    "CUDA = True\n",
    "VOCAB_SIZE = 5000\n",
    "MAX_SEQ_LEN = 20\n",
    "START_LETTER = 0\n",
    "BATCH_SIZE = 64\n",
    "MLE_TRAIN_EPOCHS = 100\n",
    "ADV_TRAIN_EPOCHS = 50\n",
    "POS_NEG_SAMPLES = 10000\n",
    "\n",
    "GEN_EMBEDDING_DIM = 32\n",
    "GEN_HIDDEN_DIM = 32\n",
    "DIS_EMBEDDING_DIM = 64\n",
    "DIS_HIDDEN_DIM = 64\n",
    "FILTER_SIZES = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "NUM_FILTERS = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\n",
    "\n",
    "POSITIVE_FILE = 'real.data'\n",
    "NEGATIVE_FILE = 'gene.data'\n",
    "\n",
    "oracle_samples_path = './oracle_samples.trc'\n",
    "oracle_state_dict_path = './oracle_EMBDIM32_HIDDENDIM32_VOCAB5000_MAXSEQLEN20.trc'\n",
    "pretrained_gen_path = './gen_MLEtrain_EMBDIM32_HIDDENDIM32_VOCAB5000_MAXSEQLEN20.trc'\n",
    "pretrained_dis_path = './dis_pretrain_EMBDIM_64_HIDDENDIM64_VOCAB5000_MAXSEQLEN20.trc'\n",
    "\n",
    "def generate_samples(model, batch_size, generated_num, output_file):\n",
    "    samples = []\n",
    "    for _ in range(int(generated_num / batch_size)):\n",
    "        sample = model.sample(batch_size).cpu().numpy().tolist()\n",
    "        samples.extend(sample)\n",
    "\n",
    "    with open(output_file, 'w') as fout:\n",
    "        for sample in samples:\n",
    "            string = ''.join([str(s) for s in sample])\n",
    "            fout.write('{}\\n'.format(string))\n",
    "\n",
    "\n",
    "\n",
    "def train_generator_MLE(gen, gen_opt, oracle, real_data_samples, epochs):\n",
    "    \"\"\"\n",
    "    Max Likelihood Pretraining for the generator\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        print('epoch %d : ' % (epoch + 1), end='')\n",
    "        sys.stdout.flush()\n",
    "        total_loss = 0\n",
    "\n",
    "        for i in range(0, POS_NEG_SAMPLES, BATCH_SIZE):\n",
    "            inp, target = helpers.prepare_generator_batch(real_data_samples[i:i + BATCH_SIZE], start_letter=START_LETTER,\n",
    "                                                          gpu=CUDA)\n",
    "            gen_opt.zero_grad()\n",
    "            loss = gen.batchNLLLoss(inp, target)\n",
    "            loss.backward()\n",
    "            gen_opt.step()\n",
    "\n",
    "            total_loss += loss.data.item()\n",
    "\n",
    "            if (i / BATCH_SIZE) % ceil(\n",
    "                            ceil(POS_NEG_SAMPLES / float(BATCH_SIZE)) / 10.) == 0:  # roughly every 10% of an epoch\n",
    "                print('.', end='')\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        # each loss in a batch is loss per sample\n",
    "        total_loss = total_loss / ceil(POS_NEG_SAMPLES / float(BATCH_SIZE)) / MAX_SEQ_LEN\n",
    "\n",
    "        # sample from generator and compute oracle NLL\n",
    "        oracle_loss = helpers.batchwise_oracle_nll(gen, oracle, POS_NEG_SAMPLES, BATCH_SIZE, MAX_SEQ_LEN,\n",
    "                                                   start_letter=START_LETTER, gpu=CUDA)\n",
    "\n",
    "        print(' average_train_NLL = %.4f, oracle_sample_NLL = %.4f' % (total_loss, oracle_loss))\n",
    "\n",
    "\n",
    "def train_generator_PG(gen, gen_opt, oracle, dis, num_batches):\n",
    "    \"\"\"\n",
    "    The generator is trained using policy gradients, using the reward from the discriminator.\n",
    "    Training is done for num_batches batches.\n",
    "    \"\"\"\n",
    "\n",
    "    for batch in range(num_batches):\n",
    "        s = gen.sample(BATCH_SIZE*2)        # 64 works best\n",
    "        inp, target = helpers.prepare_generator_batch(s, start_letter=START_LETTER, gpu=CUDA)\n",
    "        rewards = dis(target)\n",
    "        rewards = rewards.data[:,1]\n",
    "        #rewards = dis.batchClassify(target)\n",
    "|\n",
    "        gen_opt.zero_grad()\n",
    "        pg_loss = gen.batchPGLoss(inp, target, rewards)\n",
    "        pg_loss.backward()\n",
    "        gen_opt.step()\n",
    "\n",
    "    # sample from generator and compute oracle NLL\n",
    "    oracle_loss = helpers.batchwise_oracle_nll(gen, oracle, POS_NEG_SAMPLES, BATCH_SIZE, MAX_SEQ_LEN,\n",
    "                                                   start_letter=START_LETTER, gpu=CUDA)\n",
    "\n",
    "    print(' oracle_sample_NLL = %.4f' % oracle_loss)\n",
    "\n",
    "def eval_discriminator(model, data_iter, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate discriminator, dropout is enabled\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_iter:\n",
    "            if CUDA:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            target = target.contiguous().view(-1)\n",
    "            output = model(data)\n",
    "            pred = output.data.max(1)[1]\n",
    "            correct += pred.eq(target.data).cpu().sum()\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(data_iter)\n",
    "    acc = correct.item() / data_iter.data_num\n",
    "    return avg_loss, acc\n",
    "    \n",
    "def train_discriminator(discriminator, dis_opt, real_data_samples, generator, oracle, d_steps, epochs):\n",
    "    \"\"\"\n",
    "    Training the discriminator on real_data_samples (positive) and generated samples from generator (negative).\n",
    "    Samples are drawn d_steps times, and the discriminator is trained for epochs epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    # generating a small validation set before training (using oracle and generator)\n",
    "    pos_val = oracle.sample(100)\n",
    "    neg_val = generator.sample(100)\n",
    "    val_inp, val_target = helpers.prepare_discriminator_data(pos_val, neg_val, gpu=CUDA)\n",
    "\n",
    "    for d_step in range(d_steps):\n",
    "        s = helpers.batchwise_sample(generator, POS_NEG_SAMPLES, BATCH_SIZE)\n",
    "        dis_inp, dis_target = helpers.prepare_discriminator_data(real_data_samples, s, gpu=CUDA)\n",
    "        for epoch in range(epochs):\n",
    "            print('d-step %d epoch %d : ' % (d_step + 1, epoch + 1), end='')\n",
    "            sys.stdout.flush()\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "\n",
    "            for i in range(0, 2 * POS_NEG_SAMPLES, BATCH_SIZE):\n",
    "                inp, target = dis_inp[i:i + BATCH_SIZE].cpu().numpy(), dis_target[i:i + BATCH_SIZE].cpu().numpy()\n",
    "                dis_opt.zero_grad()\n",
    "                inp = torch.tensor(inp).cuda()\n",
    "                target = torch.tensor(target).cuda()\n",
    "                out = discriminator(inp)\n",
    "                loss_fn = nn.NLLLoss()\n",
    "                loss = loss_fn(out, target)\n",
    "                loss.backward()\n",
    "                dis_opt.step()\n",
    "\n",
    "                total_loss += loss.data.item()\n",
    "                total_acc += torch.sum(out.max(1)[1]==target).data.item()\n",
    "                if (i / BATCH_SIZE) % ceil(ceil(2 * POS_NEG_SAMPLES / float(\n",
    "                        BATCH_SIZE)) / 10.) == 0:  # roughly every 10% of an epoch\n",
    "                    print('.', end='')\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "            total_loss /= ceil(2 * POS_NEG_SAMPLES / float(BATCH_SIZE))\n",
    "            total_acc /= float(2 * POS_NEG_SAMPLES)\n",
    "\n",
    "            val_pred = discriminator(val_inp)\n",
    "            print(' average_loss = %.4f, train_acc = %.4f, val_acc = %.4f' % (\n",
    "                total_loss, total_acc, torch.sum(val_pred.max(1)[1]==val_target).data.item()/200.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Generator MLE Training...\n"
     ]
    }
   ],
   "source": [
    "    oracle = generator.Generator(GEN_EMBEDDING_DIM, GEN_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=CUDA)\n",
    "    oracle.load_state_dict(torch.load(oracle_state_dict_path))\n",
    "    oracle_samples = torch.load(oracle_samples_path).type(torch.LongTensor)\n",
    "    # a new oracle can be generated by passing oracle_init=True in the generator constructor\n",
    "    # samples for the new oracle can be generated using helpers.batchwise_sample()\n",
    "\n",
    "    gen = generator.Generator(GEN_EMBEDDING_DIM, GEN_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=CUDA)\n",
    "    #num_classes, vocab_size, emb_dim, filter_sizes, num_filters, dropout\n",
    "    #2, VOCAB_SIZE, DIS_EMBEDDING_DIM, [3, 4, 5], [100, 100, 100], 0.5\n",
    "    dis = discriminator_cnn.Discriminator(2, VOCAB_SIZE, DIS_EMBEDDING_DIM, FILTER_SIZES, NUM_FILTERS, 0.5)\n",
    "\n",
    "    if CUDA:\n",
    "        oracle = oracle.cuda()\n",
    "        gen = gen.cuda()\n",
    "        dis = dis.cuda()\n",
    "        oracle_samples = oracle_samples.cuda()\n",
    "\n",
    "    # GENERATOR MLE TRAINING\n",
    "    print('Starting Generator MLE Training...')\n",
    "    gen_optimizer = optim.Adam(gen.parameters(), lr=1e-2)\n",
    "    #train_generator_MLE(gen, gen_optimizer, oracle, oracle_samples, MLE_TRAIN_EPOCHS)\n",
    "\n",
    "    # torch.save(gen.state_dict(), pretrained_gen_path)\n",
    "    gen.load_state_dict(torch.load(pretrained_gen_path))\n",
    "\n",
    "    # PRETRAIN DISCRIMINATOR\n",
    "    #print('\\nStarting Discriminator Training...')\n",
    "    #dis_optimizer = optim.Adam(dis.parameters())\n",
    "    #train_discriminator(dis, dis_optimizer, oracle_samples, gen, oracle, 50, 3)\n",
    "\n",
    "    # torch.save(dis.state_dict(), pretrained_dis_path)\n",
    "    # dis.load_state_dict(torch.load(pretrained_dis_path))\n",
    "\n",
    "    # ADVERSARIAL TRAINING\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9c8a18a650>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    dis = discriminator_cnn.Discriminator(2, VOCAB_SIZE, DIS_EMBEDDING_DIM, FILTER_SIZES, NUM_FILTERS, 0.2)\n",
    "\n",
    "    if CUDA:\n",
    "        dis = dis.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Discriminator Training...\n",
      "d-step 1 epoch 1 : .."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krivas/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........ average_loss = 0.6935, train_acc = 0.4970, val_acc = 0.4800\n",
      "d-step 1 epoch 2 : .......... average_loss = 0.6934, train_acc = 0.4993, val_acc = 0.5100\n",
      "d-step 1 epoch 3 : .......... average_loss = 0.6933, train_acc = 0.4993, val_acc = 0.4600\n",
      "d-step 2 epoch 1 : .......... average_loss = 0.6932, train_acc = 0.5008, val_acc = 0.5000\n",
      "d-step 2 epoch 2 : .......... average_loss = 0.6930, train_acc = 0.5050, val_acc = 0.4900\n",
      "d-step 2 epoch 3 : .......... average_loss = 0.6930, train_acc = 0.5072, val_acc = 0.5000\n",
      "d-step 3 epoch 1 : .......... average_loss = 0.6930, train_acc = 0.5052, val_acc = 0.5550\n",
      "d-step 3 epoch 2 : .......... average_loss = 0.6929, train_acc = 0.5078, val_acc = 0.4650\n",
      "d-step 3 epoch 3 : .......... average_loss = 0.6928, train_acc = 0.5084, val_acc = 0.5400\n",
      "d-step 4 epoch 1 : .......... average_loss = 0.6930, train_acc = 0.5050, val_acc = 0.5800\n",
      "d-step 4 epoch 2 : .......... average_loss = 0.6928, train_acc = 0.5110, val_acc = 0.5250\n",
      "d-step 4 epoch 3 : .......... average_loss = 0.6927, train_acc = 0.5118, val_acc = 0.4850\n",
      "d-step 5 epoch 1 : .......... average_loss = 0.6927, train_acc = 0.5151, val_acc = 0.5000\n",
      "d-step 5 epoch 2 : .......... average_loss = 0.6926, train_acc = 0.5137, val_acc = 0.5050\n",
      "d-step 5 epoch 3 : .......... average_loss = 0.6926, train_acc = 0.5184, val_acc = 0.5050\n",
      "d-step 6 epoch 1 : .......... average_loss = 0.6925, train_acc = 0.5209, val_acc = 0.4750\n",
      "d-step 6 epoch 2 : .......... average_loss = 0.6926, train_acc = 0.5132, val_acc = 0.4650\n",
      "d-step 6 epoch 3 : .......... average_loss = 0.6925, train_acc = 0.5199, val_acc = 0.4800\n",
      "d-step 7 epoch 1 : .......... average_loss = 0.6923, train_acc = 0.5220, val_acc = 0.4800\n",
      "d-step 7 epoch 2 : .......... average_loss = 0.6923, train_acc = 0.5218, val_acc = 0.5100\n",
      "d-step 7 epoch 3 : .......... average_loss = 0.6923, train_acc = 0.5295, val_acc = 0.5300\n",
      "d-step 8 epoch 1 : .......... average_loss = 0.6922, train_acc = 0.5235, val_acc = 0.5550\n",
      "d-step 8 epoch 2 : .......... average_loss = 0.6922, train_acc = 0.5265, val_acc = 0.5400\n",
      "d-step 8 epoch 3 : .......... average_loss = 0.6920, train_acc = 0.5275, val_acc = 0.5300\n",
      "d-step 9 epoch 1 : .......... average_loss = 0.6920, train_acc = 0.5270, val_acc = 0.4900\n",
      "d-step 9 epoch 2 : .......... average_loss = 0.6917, train_acc = 0.5345, val_acc = 0.5450\n",
      "d-step 9 epoch 3 : .......... average_loss = 0.6918, train_acc = 0.5336, val_acc = 0.5400\n",
      "d-step 10 epoch 1 : .......... average_loss = 0.6922, train_acc = 0.5233, val_acc = 0.5300\n",
      "d-step 10 epoch 2 : .......... average_loss = 0.6918, train_acc = 0.5274, val_acc = 0.5000\n",
      "d-step 10 epoch 3 : .......... average_loss = 0.6916, train_acc = 0.5357, val_acc = 0.5000\n",
      "d-step 11 epoch 1 : .......... average_loss = 0.6915, train_acc = 0.5387, val_acc = 0.4850\n",
      "d-step 11 epoch 2 : .......... average_loss = 0.6913, train_acc = 0.5350, val_acc = 0.5950\n",
      "d-step 11 epoch 3 : .......... average_loss = 0.6913, train_acc = 0.5423, val_acc = 0.5350\n",
      "d-step 12 epoch 1 : .......... average_loss = 0.6911, train_acc = 0.5443, val_acc = 0.5500\n",
      "d-step 12 epoch 2 : .......... average_loss = 0.6912, train_acc = 0.5414, val_acc = 0.4700\n",
      "d-step 12 epoch 3 : .......... average_loss = 0.6911, train_acc = 0.5419, val_acc = 0.4900\n",
      "d-step 13 epoch 1 : .......... average_loss = 0.6906, train_acc = 0.5498, val_acc = 0.4900\n",
      "d-step 13 epoch 2 : .......... average_loss = 0.6907, train_acc = 0.5450, val_acc = 0.5100\n",
      "d-step 13 epoch 3 : .......... average_loss = 0.6902, train_acc = 0.5522, val_acc = 0.5050\n",
      "d-step 14 epoch 1 : .......... average_loss = 0.6904, train_acc = 0.5476, val_acc = 0.5150\n",
      "d-step 14 epoch 2 : .......... average_loss = 0.6903, train_acc = 0.5453, val_acc = 0.5450\n",
      "d-step 14 epoch 3 : .......... average_loss = 0.6897, train_acc = 0.5565, val_acc = 0.5200\n",
      "d-step 15 epoch 1 : .......... average_loss = 0.6899, train_acc = 0.5548, val_acc = 0.5350\n",
      "d-step 15 epoch 2 : .......... average_loss = 0.6895, train_acc = 0.5616, val_acc = 0.5450\n",
      "d-step 15 epoch 3 : .......... average_loss = 0.6892, train_acc = 0.5663, val_acc = 0.4750\n",
      "d-step 16 epoch 1 : .......... average_loss = 0.6893, train_acc = 0.5565, val_acc = 0.5400\n",
      "d-step 16 epoch 2 : .......... average_loss = 0.6886, train_acc = 0.5631, val_acc = 0.5200\n",
      "d-step 16 epoch 3 : .......... average_loss = 0.6886, train_acc = 0.5649, val_acc = 0.5300\n",
      "d-step 17 epoch 1 : .......... average_loss = 0.6884, train_acc = 0.5655, val_acc = 0.5900\n",
      "d-step 17 epoch 2 : .......... average_loss = 0.6878, train_acc = 0.5692, val_acc = 0.5050\n",
      "d-step 17 epoch 3 : .......... average_loss = 0.6873, train_acc = 0.5737, val_acc = 0.5450\n",
      "d-step 18 epoch 1 : .......... average_loss = 0.6873, train_acc = 0.5654, val_acc = 0.5550\n",
      "d-step 18 epoch 2 : .......... average_loss = 0.6863, train_acc = 0.5721, val_acc = 0.5300\n",
      "d-step 18 epoch 3 : .......... average_loss = 0.6861, train_acc = 0.5735, val_acc = 0.5450\n",
      "d-step 19 epoch 1 : .......... average_loss = 0.6862, train_acc = 0.5717, val_acc = 0.5450\n",
      "d-step 19 epoch 2 : .......... average_loss = 0.6858, train_acc = 0.5717, val_acc = 0.5400\n",
      "d-step 19 epoch 3 : .......... average_loss = 0.6854, train_acc = 0.5729, val_acc = 0.5350\n",
      "d-step 20 epoch 1 : .......... average_loss = 0.6848, train_acc = 0.5815, val_acc = 0.5300\n",
      "d-step 20 epoch 2 : .......... average_loss = 0.6842, train_acc = 0.5805, val_acc = 0.5350\n",
      "d-step 20 epoch 3 : .......... average_loss = 0.6835, train_acc = 0.5886, val_acc = 0.5800\n",
      "d-step 21 epoch 1 : .......... average_loss = 0.6829, train_acc = 0.5844, val_acc = 0.5150\n",
      "d-step 21 epoch 2 : .......... average_loss = 0.6823, train_acc = 0.5870, val_acc = 0.5350\n",
      "d-step 21 epoch 3 : .......... average_loss = 0.6813, train_acc = 0.5897, val_acc = 0.5750\n",
      "d-step 22 epoch 1 : .......... average_loss = 0.6801, train_acc = 0.5931, val_acc = 0.5600\n",
      "d-step 22 epoch 2 : .......... average_loss = 0.6790, train_acc = 0.5959, val_acc = 0.5950\n",
      "d-step 22 epoch 3 : .......... average_loss = 0.6782, train_acc = 0.5958, val_acc = 0.5700\n",
      "d-step 23 epoch 1 : .......... average_loss = 0.6776, train_acc = 0.5948, val_acc = 0.5650\n",
      "d-step 23 epoch 2 : .......... average_loss = 0.6764, train_acc = 0.5990, val_acc = 0.5950\n",
      "d-step 23 epoch 3 : .......... average_loss = 0.6749, train_acc = 0.5981, val_acc = 0.5700\n",
      "d-step 24 epoch 1 : .......... average_loss = 0.6730, train_acc = 0.6081, val_acc = 0.5800\n",
      "d-step 24 epoch 2 : .......... average_loss = 0.6714, train_acc = 0.6089, val_acc = 0.5550\n",
      "d-step 24 epoch 3 : .......... average_loss = 0.6699, train_acc = 0.6118, val_acc = 0.5750\n",
      "d-step 25 epoch 1 : .......... average_loss = 0.6704, train_acc = 0.6061, val_acc = 0.6000\n",
      "d-step 25 epoch 2 : .......... average_loss = 0.6685, train_acc = 0.6073, val_acc = 0.6000\n",
      "d-step 25 epoch 3 : .......... average_loss = 0.6665, train_acc = 0.6125, val_acc = 0.5800\n",
      "d-step 26 epoch 1 : .......... average_loss = 0.6658, train_acc = 0.6112, val_acc = 0.6000\n",
      "d-step 26 epoch 2 : .......... average_loss = 0.6633, train_acc = 0.6186, val_acc = 0.6000\n",
      "d-step 26 epoch 3 : .......... average_loss = 0.6614, train_acc = 0.6201, val_acc = 0.6200\n",
      "d-step 27 epoch 1 : .......... average_loss = 0.6598, train_acc = 0.6187, val_acc = 0.6250\n",
      "d-step 27 epoch 2 : .......... average_loss = 0.6572, train_acc = 0.6250, val_acc = 0.6200\n",
      "d-step 27 epoch 3 : .......... average_loss = 0.6538, train_acc = 0.6278, val_acc = 0.6350\n",
      "d-step 28 epoch 1 : .......... average_loss = 0.6511, train_acc = 0.6310, val_acc = 0.5950\n",
      "d-step 28 epoch 2 : .......... average_loss = 0.6476, train_acc = 0.6357, val_acc = 0.6100\n",
      "d-step 28 epoch 3 : .......... average_loss = 0.6442, train_acc = 0.6392, val_acc = 0.6300\n",
      "d-step 29 epoch 1 : .......... average_loss = 0.6410, train_acc = 0.6409, val_acc = 0.6350\n",
      "d-step 29 epoch 2 : .......... average_loss = 0.6366, train_acc = 0.6477, val_acc = 0.6500\n",
      "d-step 29 epoch 3 : .......... average_loss = 0.6331, train_acc = 0.6485, val_acc = 0.6500\n",
      "d-step 30 epoch 1 : .......... average_loss = 0.6348, train_acc = 0.6450, val_acc = 0.6550\n",
      "d-step 30 epoch 2 : .......... average_loss = 0.6306, train_acc = 0.6499, val_acc = 0.6600\n",
      "d-step 30 epoch 3 : .......... average_loss = 0.6260, train_acc = 0.6569, val_acc = 0.6250\n",
      "d-step 31 epoch 1 : .......... average_loss = 0.6234, train_acc = 0.6615, val_acc = 0.6350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d-step 31 epoch 2 : .......... average_loss = 0.6187, train_acc = 0.6661, val_acc = 0.6400\n",
      "d-step 31 epoch 3 : .......... average_loss = 0.6135, train_acc = 0.6710, val_acc = 0.6150\n",
      "d-step 32 epoch 1 : .......... average_loss = 0.6146, train_acc = 0.6671, val_acc = 0.6450\n",
      "d-step 32 epoch 2 : .......... average_loss = 0.6089, train_acc = 0.6737, val_acc = 0.6350\n",
      "d-step 32 epoch 3 : .......... average_loss = 0.6035, train_acc = 0.6823, val_acc = 0.6450\n",
      "d-step 33 epoch 1 : .......... average_loss = 0.6064, train_acc = 0.6742, val_acc = 0.6350\n",
      "d-step 33 epoch 2 : .......... average_loss = 0.6009, train_acc = 0.6818, val_acc = 0.6650\n",
      "d-step 33 epoch 3 : .......... average_loss = 0.5953, train_acc = 0.6883, val_acc = 0.6400\n",
      "d-step 34 epoch 1 : .......... average_loss = 0.5970, train_acc = 0.6810, val_acc = 0.6350\n",
      "d-step 34 epoch 2 : .......... average_loss = 0.5911, train_acc = 0.6878, val_acc = 0.6200\n",
      "d-step 34 epoch 3 : .......... average_loss = 0.5851, train_acc = 0.6939, val_acc = 0.6200\n",
      "d-step 35 epoch 1 : .......... average_loss = 0.5826, train_acc = 0.6940, val_acc = 0.6200\n",
      "d-step 35 epoch 2 : .......... average_loss = 0.5762, train_acc = 0.7037, val_acc = 0.6150\n",
      "d-step 35 epoch 3 : .......... average_loss = 0.5691, train_acc = 0.7102, val_acc = 0.6350\n",
      "d-step 36 epoch 1 : .......... average_loss = 0.5729, train_acc = 0.7042, val_acc = 0.6550\n",
      "d-step 36 epoch 2 : .......... average_loss = 0.5644, train_acc = 0.7130, val_acc = 0.6300\n",
      "d-step 36 epoch 3 : .......... average_loss = 0.5575, train_acc = 0.7190, val_acc = 0.6450\n",
      "d-step 37 epoch 1 : .......... average_loss = 0.5704, train_acc = 0.7042, val_acc = 0.6450\n",
      "d-step 37 epoch 2 : .......... average_loss = 0.5611, train_acc = 0.7137, val_acc = 0.6450\n",
      "d-step 37 epoch 3 : .......... average_loss = 0.5529, train_acc = 0.7187, val_acc = 0.6400\n",
      "d-step 38 epoch 1 : .......... average_loss = 0.5572, train_acc = 0.7180, val_acc = 0.6500\n",
      "d-step 38 epoch 2 : .......... average_loss = 0.5488, train_acc = 0.7280, val_acc = 0.6500\n",
      "d-step 38 epoch 3 : .......... average_loss = 0.5391, train_acc = 0.7329, val_acc = 0.6350\n",
      "d-step 39 epoch 1 : .......... average_loss = 0.5419, train_acc = 0.7323, val_acc = 0.6200\n",
      "d-step 39 epoch 2 : .......... average_loss = 0.5314, train_acc = 0.7396, val_acc = 0.6450\n",
      "d-step 39 epoch 3 : .......... average_loss = 0.5214, train_acc = 0.7460, val_acc = 0.6150\n",
      "d-step 40 epoch 1 : .......... average_loss = 0.5280, train_acc = 0.7393, val_acc = 0.6450\n",
      "d-step 40 epoch 2 : .......... average_loss = 0.5153, train_acc = 0.7500, val_acc = 0.6500\n",
      "d-step 40 epoch 3 : .......... average_loss = 0.5049, train_acc = 0.7586, val_acc = 0.6450\n",
      "d-step 41 epoch 1 : .......... average_loss = 0.5147, train_acc = 0.7528, val_acc = 0.6050\n",
      "d-step 41 epoch 2 : .......... average_loss = 0.5009, train_acc = 0.7648, val_acc = 0.6200\n",
      "d-step 41 epoch 3 : .......... average_loss = 0.4887, train_acc = 0.7738, val_acc = 0.6200\n",
      "d-step 42 epoch 1 : .......... average_loss = 0.4973, train_acc = 0.7654, val_acc = 0.6200\n",
      "d-step 42 epoch 2 : .......... average_loss = 0.4808, train_acc = 0.7769, val_acc = 0.6150\n",
      "d-step 42 epoch 3 : .......... average_loss = 0.4659, train_acc = 0.7909, val_acc = 0.6150\n",
      "d-step 43 epoch 1 : .......... average_loss = 0.4860, train_acc = 0.7726, val_acc = 0.6050\n",
      "d-step 43 epoch 2 : .......... average_loss = 0.4683, train_acc = 0.7875, val_acc = 0.6200\n",
      "d-step 43 epoch 3 : .......... average_loss = 0.4521, train_acc = 0.8009, val_acc = 0.6000\n",
      "d-step 44 epoch 1 : .......... average_loss = 0.4677, train_acc = 0.7839, val_acc = 0.5900\n",
      "d-step 44 epoch 2 : .......... average_loss = 0.4490, train_acc = 0.8005, val_acc = 0.5850\n",
      "d-step 44 epoch 3 : .......... average_loss = 0.4318, train_acc = 0.8132, val_acc = 0.6000\n",
      "d-step 45 epoch 1 : .......... average_loss = 0.4464, train_acc = 0.8006, val_acc = 0.5850\n",
      "d-step 45 epoch 2 : .......... average_loss = 0.4251, train_acc = 0.8179, val_acc = 0.5950\n",
      "d-step 45 epoch 3 : .......... average_loss = 0.4051, train_acc = 0.8280, val_acc = 0.6050\n",
      "d-step 46 epoch 1 : .......... average_loss = 0.4301, train_acc = 0.8124, val_acc = 0.5900\n",
      "d-step 46 epoch 2 : .......... average_loss = 0.4058, train_acc = 0.8295, val_acc = 0.5900\n",
      "d-step 46 epoch 3 : .......... average_loss = 0.3854, train_acc = 0.8443, val_acc = 0.5900\n",
      "d-step 47 epoch 1 : .......... average_loss = 0.4059, train_acc = 0.8284, val_acc = 0.5850\n",
      "d-step 47 epoch 2 : .......... average_loss = 0.3796, train_acc = 0.8476, val_acc = 0.5700\n",
      "d-step 47 epoch 3 : .......... average_loss = 0.3565, train_acc = 0.8637, val_acc = 0.5650\n",
      "d-step 48 epoch 1 : .......... average_loss = 0.3931, train_acc = 0.8391, val_acc = 0.5600\n",
      "d-step 48 epoch 2 : .......... average_loss = 0.3641, train_acc = 0.8576, val_acc = 0.5750\n",
      "d-step 48 epoch 3 : .......... average_loss = 0.3382, train_acc = 0.8728, val_acc = 0.5800\n",
      "d-step 49 epoch 1 : .......... average_loss = 0.3680, train_acc = 0.8560, val_acc = 0.5900\n",
      "d-step 49 epoch 2 : .......... average_loss = 0.3366, train_acc = 0.8734, val_acc = 0.5500\n",
      "d-step 49 epoch 3 : .......... average_loss = 0.3100, train_acc = 0.8897, val_acc = 0.5700\n",
      "d-step 50 epoch 1 : .......... average_loss = 0.3533, train_acc = 0.8649, val_acc = 0.5850\n",
      "d-step 50 epoch 2 : .......... average_loss = 0.3182, train_acc = 0.8869, val_acc = 0.5800\n",
      "d-step 50 epoch 3 : .......... average_loss = 0.2911, train_acc = 0.9012, val_acc = 0.5750\n"
     ]
    }
   ],
   "source": [
    "    # PRETRAIN DISCRIMINATOR\n",
    "    print('\\nStarting Discriminator Training...')\n",
    "    dis_optimizer = optim.SGD(dis.parameters(), lr=1e-2)\n",
    "    train_discriminator(dis, dis_optimizer, oracle_samples, gen, oracle, 50, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Adversarial Training...\n",
      "\n",
      "Initial Oracle Sample Loss : 10.9589\n",
      "\n",
      "--------\n",
      "EPOCH 1\n",
      "--------\n",
      "\n",
      "Adversarial Training Generator : "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krivas/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " oracle_sample_NLL = 11.0608\n",
      "\n",
      "Adversarial Training Discriminator : \n",
      "d-step 1 epoch 1 : .......... average_loss = 0.3325, train_acc = 0.8789, val_acc = 0.6700\n",
      "d-step 1 epoch 2 : .......... average_loss = 0.2954, train_acc = 0.8990, val_acc = 0.6600\n",
      "d-step 1 epoch 3 : .......... average_loss = 0.2657, train_acc = 0.9163, val_acc = 0.6600\n",
      "d-step 2 epoch 1 : .......... average_loss = 0.3144, train_acc = 0.8886, val_acc = 0.6750\n",
      "d-step 2 epoch 2 : .......... average_loss = 0.2756, train_acc = 0.9100, val_acc = 0.6600\n",
      "d-step 2 epoch 3 : .......... average_loss = 0.2450, train_acc = 0.9267, val_acc = 0.6600\n",
      "d-step 3 epoch 1 : .......... average_loss = 0.2908, train_acc = 0.9013, val_acc = 0.6650\n",
      "d-step 3 epoch 2 : .......... average_loss = 0.2515, train_acc = 0.9225, val_acc = 0.6850\n",
      "d-step 3 epoch 3 : .......... average_loss = 0.2198, train_acc = 0.9379, val_acc = 0.6800\n",
      "d-step 4 epoch 1 : .......... average_loss = 0.2891, train_acc = 0.9040, val_acc = 0.6700\n",
      "d-step 4 epoch 2 : .......... average_loss = 0.2450, train_acc = 0.9266, val_acc = 0.6800\n",
      "d-step 4 epoch 3 : .......... average_loss = 0.2134, train_acc = 0.9414, val_acc = 0.6800\n",
      "d-step 5 epoch 1 : .......... average_loss = 0.2656, train_acc = 0.9143, val_acc = 0.6500\n",
      "d-step 5 epoch 2 : .......... average_loss = 0.2211, train_acc = 0.9374, val_acc = 0.6600\n",
      "d-step 5 epoch 3 : .......... average_loss = 0.1901, train_acc = 0.9514, val_acc = 0.6450\n",
      "\n",
      "--------\n",
      "EPOCH 2\n",
      "--------\n",
      "\n",
      "Adversarial Training Generator :  oracle_sample_NLL = 11.1203\n",
      "\n",
      "Adversarial Training Discriminator : \n",
      "d-step 1 epoch 1 : .......... average_loss = 0.2583, train_acc = 0.9203, val_acc = 0.5750\n",
      "d-step 1 epoch 2 : .......... average_loss = 0.2091, train_acc = 0.9425, val_acc = 0.5750\n",
      "d-step 1 epoch 3 : .......... average_loss = 0.1762, train_acc = 0.9564, val_acc = 0.5950\n",
      "d-step 2 epoch 1 : .......... average_loss = 0.2478, train_acc = 0.9231, val_acc = 0.5400\n",
      "d-step 2 epoch 2 : .......... average_loss = 0.2001, train_acc = 0.9473, val_acc = 0.5500\n",
      "d-step 2 epoch 3 : .......... average_loss = 0.1666, train_acc = 0.9597, val_acc = 0.5600\n",
      "d-step 3 epoch 1 : .......... average_loss = 0.2359, train_acc = 0.9295, val_acc = 0.5400\n",
      "d-step 3 epoch 2 : .......... average_loss = 0.1851, train_acc = 0.9506, val_acc = 0.5550\n",
      "d-step 3 epoch 3 : .......... average_loss = 0.1520, train_acc = 0.9645, val_acc = 0.5500\n",
      "d-step 4 epoch 1 : .......... average_loss = 0.2165, train_acc = 0.9365, val_acc = 0.5400\n",
      "d-step 4 epoch 2 : .......... average_loss = 0.1675, train_acc = 0.9567, val_acc = 0.5350\n",
      "d-step 4 epoch 3 : .......... average_loss = 0.1347, train_acc = 0.9701, val_acc = 0.5500\n",
      "d-step 5 epoch 1 : .......... average_loss = 0.2133, train_acc = 0.9368, val_acc = 0.5350\n",
      "d-step 5 epoch 2 : .......... average_loss = 0.1612, train_acc = 0.9583, val_acc = 0.5150\n",
      "d-step 5 epoch 3 : .......... average_loss = 0.1272, train_acc = 0.9731, val_acc = 0.5300\n",
      "\n",
      "--------\n",
      "EPOCH 3\n",
      "--------\n",
      "\n",
      "Adversarial Training Generator :  oracle_sample_NLL = 11.1549\n",
      "\n",
      "Adversarial Training Discriminator : \n",
      "d-step 1 epoch 1 : .......... average_loss = 0.2048, train_acc = 0.9408, val_acc = 0.5700\n",
      "d-step 1 epoch 2 : .......... average_loss = 0.1528, train_acc = 0.9628, val_acc = 0.6000\n",
      "d-step 1 epoch 3 : .......... average_loss = 0.1189, train_acc = 0.9756, val_acc = 0.6000\n",
      "d-step 2 epoch 1 : .......... average_loss = 0.1927, train_acc = 0.9438, val_acc = 0.6100\n",
      "d-step 2 epoch 2 : .......... average_loss = 0.1403, train_acc = 0.9658, val_acc = 0.6050\n",
      "d-step 2 epoch 3 : .......... average_loss = 0.1075, train_acc = 0.9785, val_acc = 0.6050\n",
      "d-step 3 epoch 1 : .......... average_loss = 0.1830, train_acc = 0.9498, val_acc = 0.5950\n",
      "d-step 3 epoch 2 : .......... average_loss = 0.1304, train_acc = 0.9696, val_acc = 0.6100\n",
      "d-step 3 epoch 3 : .......... average_loss = 0.0979, train_acc = 0.9808, val_acc = 0.6050\n",
      "d-step 4 epoch 1 : .......... average_loss = 0.1862, train_acc = 0.9476, val_acc = 0.6050\n",
      "d-step 4 epoch 2 : .......... average_loss = 0.1320, train_acc = 0.9686, val_acc = 0.6150\n",
      "d-step 4 epoch 3 : .......... average_loss = 0.0989, train_acc = 0.9798, val_acc = 0.6050\n",
      "d-step 5 epoch 1 : .......... average_loss = 0.1718, train_acc = 0.9525, val_acc = 0.5900\n",
      "d-step 5 epoch 2 : .......... average_loss = 0.1195, train_acc = 0.9719, val_acc = 0.5900\n",
      "d-step 5 epoch 3 : .......... average_loss = 0.0871, train_acc = 0.9839, val_acc = 0.5950\n",
      "\n",
      "--------\n",
      "EPOCH 4\n",
      "--------\n",
      "\n",
      "Adversarial Training Generator :  oracle_sample_NLL = 11.2427\n",
      "\n",
      "Adversarial Training Discriminator : \n",
      "d-step 1 epoch 1 : .......... average_loss = 0.1588, train_acc = 0.9558, val_acc = 0.5650\n",
      "d-step 1 epoch 2 : .......... average_loss = 0.1052, train_acc = 0.9771, val_acc = 0.5650\n",
      "d-step 1 epoch 3 : .......... average_loss = 0.0745, train_acc = 0.9868, val_acc = 0.5800\n",
      "d-step 2 epoch 1 : .......... average_loss = 0.1651, train_acc = 0.9543, val_acc = 0.5600\n",
      "d-step 2 epoch 2 : .......... average_loss = 0.1113, train_acc = 0.9742, val_acc = 0.5500\n",
      "d-step 2 epoch 3 : .......... average_loss = 0.0787, train_acc = 0.9847, val_acc = 0.5600\n",
      "d-step 3 epoch 1 : .......... average_loss = 0.1568, train_acc = 0.9571, val_acc = 0.5500\n",
      "d-step 3 epoch 2 : .......... average_loss = 0.1016, train_acc = 0.9768, val_acc = 0.5650\n",
      "d-step 3 epoch 3 : .......... average_loss = 0.0710, train_acc = 0.9875, val_acc = 0.5650\n",
      "d-step 4 epoch 1 : .......... average_loss = 0.1478, train_acc = 0.9596, val_acc = 0.5700\n",
      "d-step 4 epoch 2 : .......... average_loss = 0.0936, train_acc = 0.9798, val_acc = 0.5800\n",
      "d-step 4 epoch 3 : .......... average_loss = 0.0639, train_acc = 0.9897, val_acc = 0.5800\n",
      "d-step 5 epoch 1 : .......... average_loss = 0.1423, train_acc = 0.9628, val_acc = 0.5800\n",
      "d-step 5 epoch 2 : .......... average_loss = 0.0903, train_acc = 0.9807, val_acc = 0.5750\n",
      "d-step 5 epoch 3 : .......... average_loss = 0.0626, train_acc = 0.9889, val_acc = 0.5850\n",
      "\n",
      "--------\n",
      "EPOCH 5\n",
      "--------\n",
      "\n",
      "Adversarial Training Generator :  oracle_sample_NLL = 11.2188\n",
      "\n",
      "Adversarial Training Discriminator : \n",
      "d-step 1 epoch 1 : .......... average_loss = 0.1391, train_acc = 0.9621, val_acc = 0.5950\n",
      "d-step 1 epoch 2 : .......... average_loss = 0.0839, train_acc = 0.9815, val_acc = 0.6000\n",
      "d-step 1 epoch 3 : .......... average_loss = 0.0557, train_acc = 0.9911, val_acc = 0.6300\n",
      "d-step 2 epoch 1 : .......... average_loss = 0.1367, train_acc = 0.9641, val_acc = 0.6050\n",
      "d-step 2 epoch 2 : .......... average_loss = 0.0826, train_acc = 0.9823, val_acc = 0.6300\n",
      "d-step 2 epoch 3 : .......... average_loss = 0.0552, train_acc = 0.9921, val_acc = 0.6300\n",
      "d-step 3 epoch 1 : .......... average_loss = 0.1277, train_acc = 0.9660, val_acc = 0.6250\n",
      "d-step 3 epoch 2 : .......... average_loss = 0.0743, train_acc = 0.9846, val_acc = 0.6450\n",
      "d-step 3 epoch 3 : .......... average_loss = 0.0494, train_acc = 0.9927, val_acc = 0.6450\n",
      "d-step 4 epoch 1 : .......... average_loss = 0.1324, train_acc = 0.9659, val_acc = 0.6250\n",
      "d-step 4 epoch 2 : .......... average_loss = 0.0780, train_acc = 0.9826, val_acc = 0.6450\n",
      "d-step 4 epoch 3 : .......... average_loss = 0.0507, train_acc = 0.9926, val_acc = 0.6350\n",
      "d-step 5 epoch 1 : .......... average_loss = 0.1195, train_acc = 0.9675, val_acc = 0.5750\n",
      "d-step 5 epoch 2 : .......... average_loss = 0.0665, train_acc = 0.9859, val_acc = 0.6100\n",
      "d-step 5 epoch 3 : .......... average_loss = 0.0407, train_acc = 0.9940, val_acc = 0.6400\n",
      "\n",
      "--------\n",
      "EPOCH 6\n",
      "--------\n",
      "\n",
      "Adversarial Training Generator :  oracle_sample_NLL = 11.2953\n",
      "\n",
      "Adversarial Training Discriminator : \n",
      "d-step 1 epoch 1 : .......... average_loss = 0.1233, train_acc = 0.9688, val_acc = 0.5300\n",
      "d-step 1 epoch 2 : .......... average_loss = 0.0722, train_acc = 0.9846, val_acc = 0.5350\n",
      "d-step 1 epoch 3 : .......... average_loss = 0.0461, train_acc = 0.9925, val_acc = 0.5300\n",
      "d-step 2 epoch 1 : .......... average_loss = 0.1104, train_acc = 0.9709, val_acc = 0.5450\n",
      "d-step 2 epoch 2 : .......... average_loss = 0.0621, train_acc = 0.9876, val_acc = 0.5350\n",
      "d-step 2 epoch 3 : .......... average_loss = 0.0391, train_acc = 0.9946, val_acc = 0.5500\n",
      "d-step 3 epoch 1 : .......... average_loss = 0.1101, train_acc = 0.9721, val_acc = 0.5450\n",
      "d-step 3 epoch 2 : .......... average_loss = 0.0597, train_acc = 0.9882, val_acc = 0.5500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d-step 3 epoch 3 : .......... average_loss = 0.0365, train_acc = 0.9954, val_acc = 0.5450\n",
      "d-step 4 epoch 1 : .......... average_loss = 0.1023, train_acc = 0.9736, val_acc = 0.5300\n",
      "d-step 4 epoch 2 : .......... average_loss = 0.0556, train_acc = 0.9895, val_acc = 0.5350\n",
      "d-step 4 epoch 3 : .......... average_loss = 0.0346, train_acc = 0.9958, val_acc = 0.5300\n",
      "d-step 5 epoch 1 : .......... average_loss = 0.1067, train_acc = 0.9739, val_acc = 0.5300\n",
      "d-step 5 epoch 2 : .......... average_loss = 0.0593, train_acc = 0.9881, val_acc = 0.5300\n",
      "d-step 5 epoch 3 : .......... average_loss = 0.0376, train_acc = 0.9947, val_acc = 0.5350\n",
      "\n",
      "--------\n",
      "EPOCH 7\n",
      "--------\n",
      "\n",
      "Adversarial Training Generator :  oracle_sample_NLL = 11.2863\n",
      "\n",
      "Adversarial Training Discriminator : \n",
      "d-step 1 epoch 1 : .......... average_loss = 0.1022, train_acc = 0.9734, val_acc = 0.5600\n",
      "d-step 1 epoch 2 : .......... average_loss = 0.0521, train_acc = 0.9902, val_acc = 0.5500\n",
      "d-step 1 epoch 3 : .......... average_loss = 0.0321, train_acc = 0.9956, val_acc = 0.5700\n",
      "d-step 2 epoch 1 : .......... average_loss = 0.1026, train_acc = 0.9737, val_acc = 0.5650\n",
      "d-step 2 epoch 2 : .......... average_loss = 0.0531, train_acc = 0.9890, val_acc = 0.5600\n",
      "d-step 2 epoch 3 : .......... average_loss = 0.0315, train_acc = 0.9955, val_acc = 0.5700\n",
      "d-step 3 epoch 1 : .......... average_loss = 0.0993, train_acc = 0.9745, val_acc = 0.5650\n",
      "d-step 3 epoch 2 : .......... average_loss = 0.0520, train_acc = 0.9901, val_acc = 0.5700\n",
      "d-step 3 epoch 3 : .......... average_loss = 0.0305, train_acc = 0.9958, val_acc = 0.5800\n",
      "d-step 4 epoch 1 : .......... average_loss = 0.1004, train_acc = 0.9744, val_acc = 0.5900\n",
      "d-step 4 epoch 2 : .......... average_loss = 0.0513, train_acc = 0.9896, val_acc = 0.5900\n",
      "d-step 4 epoch 3 : .......... average_loss = 0.0302, train_acc = 0.9958, val_acc = 0.5800\n",
      "d-step 5 epoch 1 : .......... average_loss = 0.0947, train_acc = 0.9761, val_acc = 0.5600\n",
      "d-step 5 epoch 2 : .......... average_loss = 0.0475, train_acc = 0.9905, val_acc = 0.5700\n",
      "d-step 5 epoch 3 : .......... average_loss = 0.0272, train_acc = 0.9967, val_acc = 0.5850\n",
      "\n",
      "--------\n",
      "EPOCH 8\n",
      "--------\n",
      "\n",
      "Adversarial Training Generator :  oracle_sample_NLL = 11.3024\n",
      "\n",
      "Adversarial Training Discriminator : \n",
      "d-step 1 epoch 1 : .......... average_loss = 0.0924, train_acc = 0.9755, val_acc = 0.5950\n",
      "d-step 1 epoch 2 : .......... average_loss = 0.0441, train_acc = 0.9906, val_acc = 0.6100\n",
      "d-step 1 epoch 3 : .......... average_loss = 0.0248, train_acc = 0.9969, val_acc = 0.6200\n",
      "d-step 2 epoch 1 : .......... average_loss = 0.0897, train_acc = 0.9775, val_acc = 0.6100\n",
      "d-step 2 epoch 2 : .......... average_loss = 0.0437, train_acc = 0.9909, val_acc = 0.6000\n",
      "d-step 2 epoch 3 : .......... average_loss = 0.0244, train_acc = 0.9969, val_acc = 0.6000\n",
      "d-step 3 epoch 1 : .......... average_loss = 0.0892, train_acc = 0.9772, val_acc = 0.5950\n",
      "d-step 3 epoch 2 : .......... average_loss = 0.0447, train_acc = 0.9911, val_acc = 0.6000\n",
      "d-step 3 epoch 3 : .......... average_loss = 0.0263, train_acc = 0.9962, val_acc = 0.6050\n",
      "d-step 4 epoch 1 : .......... average_loss = 0.0856, train_acc = 0.9784, val_acc = 0.5950\n",
      "d-step 4 epoch 2 : .......... average_loss = 0.0423, train_acc = 0.9919, val_acc = 0.6000\n",
      "d-step 4 epoch 3 : .......... average_loss = 0.0234, train_acc = 0.9969, val_acc = 0.6050\n",
      "d-step 5 epoch 1 : .......... average_loss = 0.0827, train_acc = 0.9800, val_acc = 0.6000\n",
      "d-step 5 epoch 2 : .......... average_loss = 0.0409, train_acc = 0.9919, val_acc = 0.6150\n",
      "d-step 5 epoch 3 : .......... average_loss = 0.0235, train_acc = 0.9971, val_acc = 0.6150\n",
      "\n",
      "--------\n",
      "EPOCH 9\n",
      "--------\n",
      "\n",
      "Adversarial Training Generator :  oracle_sample_NLL = 11.3575\n",
      "\n",
      "Adversarial Training Discriminator : \n",
      "d-step 1 epoch 1 : .......... average_loss = 0.0858, train_acc = 0.9784, val_acc = 0.5850\n",
      "d-step 1 epoch 2 : .......... average_loss = 0.0419, train_acc = 0.9916, val_acc = 0.5950\n",
      "d-step 1 epoch 3 : .......... average_loss = 0.0235, train_acc = 0.9967, val_acc = 0.6050\n",
      "d-step 2 epoch 1 : .......... average_loss = 0.0830, train_acc = 0.9792, val_acc = 0.5950\n",
      "d-step 2 epoch 2 : .......... average_loss = 0.0391, train_acc = 0.9923, val_acc = 0.5950\n",
      "d-step 2 epoch 3 : .......... average_loss = 0.0218, train_acc = 0.9971, val_acc = 0.6000\n",
      "d-step 3 epoch 1 : .......... average_loss = 0.0770, train_acc = 0.9815, val_acc = 0.6000\n",
      "d-step 3 epoch 2 : .......... average_loss = 0.0370, train_acc = 0.9930, val_acc = 0.6150\n",
      "d-step 3 epoch 3 : .......... average_loss = 0.0202, train_acc = 0.9970, val_acc = 0.6150\n",
      "d-step 4 epoch 1 : .......... average_loss = 0.0769, train_acc = 0.9801, val_acc = 0.6100\n",
      "d-step 4 epoch 2 : .......... average_loss = 0.0338, train_acc = 0.9933, val_acc = 0.6000\n",
      "d-step 4 epoch 3 : .......... average_loss = 0.0175, train_acc = 0.9979, val_acc = 0.6050\n",
      "d-step 5 epoch 1 : .......... average_loss = 0.0706, train_acc = 0.9820, val_acc = 0.5800\n",
      "d-step 5 epoch 2 : .......... average_loss = 0.0303, train_acc = 0.9946, val_acc = 0.5850\n",
      "d-step 5 epoch 3 : .......... average_loss = 0.0164, train_acc = 0.9980, val_acc = 0.6100\n",
      "\n",
      "--------\n",
      "EPOCH 10\n",
      "--------\n",
      "\n",
      "Adversarial Training Generator :  oracle_sample_NLL = 11.3266\n",
      "\n",
      "Adversarial Training Discriminator : \n",
      "d-step 1 epoch 1 : .......... average_loss = 0.0716, train_acc = 0.9827, val_acc = 0.5850\n",
      "d-step 1 epoch 2 : .......... average_loss = 0.0318, train_acc = 0.9936, val_acc = 0.5900\n",
      "d-step 1 epoch 3 : .......... average_loss = 0.0169, train_acc = 0.9980, val_acc = 0.5900\n",
      "d-step 2 epoch 1 : .......... average_loss = 0.0716, train_acc = 0.9826, val_acc = 0.5650\n",
      "d-step 2 epoch 2 : .......... average_loss = 0.0312, train_acc = 0.9942, val_acc = 0.5900\n",
      "d-step 2 epoch 3 : .......... average_loss = 0.0171, train_acc = 0.9982, val_acc = 0.5700\n",
      "d-step 3 epoch 1 : .......... average_loss = 0.0698, train_acc = 0.9835, val_acc = 0.5800\n",
      "d-step 3 epoch 2 : .......... average_loss = 0.0296, train_acc = 0.9952, val_acc = 0.5850\n",
      "d-step 3 epoch 3 : .......... average_loss = 0.0166, train_acc = 0.9983, val_acc = 0.5950\n"
     ]
    }
   ],
   "source": [
    "    print('\\nStarting Adversarial Training...')\n",
    "    oracle_loss = helpers.batchwise_oracle_nll(gen, oracle, POS_NEG_SAMPLES, BATCH_SIZE, MAX_SEQ_LEN,\n",
    "                                               start_letter=START_LETTER, gpu=CUDA)\n",
    "    print('\\nInitial Oracle Sample Loss : %.4f' % oracle_loss)\n",
    "\n",
    "    for epoch in range(ADV_TRAIN_EPOCHS):\n",
    "        print('\\n--------\\nEPOCH %d\\n--------' % (epoch+1))\n",
    "        # TRAIN GENERATOR\n",
    "        print('\\nAdversarial Training Generator : ', end='')\n",
    "        sys.stdout.flush()\n",
    "        train_generator_PG(gen, gen_optimizer, oracle, dis, 1)\n",
    "\n",
    "        # TRAIN DISCRIMINATOR\n",
    "        print('\\nAdversarial Training Discriminator : ')\n",
    "        train_discriminator(dis, dis_optimizer, oracle_samples, gen, oracle, 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
